{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c3c910a",
   "metadata": {},
   "source": [
    "# Implement Neural Network In Python\n",
    "video number 13 from Codebasics Deep Learning Playlist\n",
    "\n",
    "video = \"https://youtu.be/PQCE9ChuIDY?si=JpL4QLruhGZdCa4C\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea46f6f3",
   "metadata": {},
   "source": [
    "We have implemenmted Activation function and loss function from scratch previously and also implemented Gradient Descent from scratch in previous Tutorial, now in this Tutorial, we will implement Neural Network from scratch, this video is basically the contininuation of the previous video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee7852",
   "metadata": {},
   "source": [
    "So our goal is to build a Neural Network class from scratch where we can do training and also do prediction without the use of any Tensorflow or keras!\n",
    "\n",
    "Now, first of all, we need the same code of Logloss, Sigmoid and Gradient Descent we did from previous video and then create a class for Neural Network, first lets write the same code again from previous video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d583f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82e4e136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>affordibility</th>\n",
       "      <th>bought_insurance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  affordibility  bought_insurance\n",
       "0    22              1                 0\n",
       "1    25              0                 0\n",
       "2    47              1                 1\n",
       "3    52              0                 0\n",
       "4    46              1                 1\n",
       "5    56              1                 1\n",
       "6    55              0                 0\n",
       "7    60              0                 1\n",
       "8    62              1                 1\n",
       "9    61              1                 1\n",
       "10   18              1                 0\n",
       "11   28              1                 0\n",
       "12   27              0                 0\n",
       "13   29              0                 0\n",
       "14   49              1                 1\n",
       "15   55              1                 1\n",
       "16   25              0                 1\n",
       "17   58              1                 1\n",
       "18   19              0                 0\n",
       "19   18              1                 0\n",
       "20   21              1                 0\n",
       "21   26              0                 0\n",
       "22   40              1                 1\n",
       "23   45              1                 1\n",
       "24   50              1                 1\n",
       "25   54              1                 1\n",
       "26   23              1                 0\n",
       "27   46              1                 0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\User\\\\OneDrive\\\\Documents\\\\Faiz Khan Program\\\\DEEP LEARNING\\\\CODEBASICS\\\\datas\\\\insurance_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7efa753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['bought_insurance'], axis=1), \n",
    "                                                   df['bought_insurance'], test_size=0.2, \n",
    "                                                   random_state=42)\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled['age'] = X_train_scaled['age'] / 100\n",
    "\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled['age'] = X_test_scaled['age'] / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c6766f",
   "metadata": {},
   "source": [
    "# Log Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14eb1b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(y_truth, y_predicted):\n",
    "    epsilon = 1e-15\n",
    "    y_predicted_new = [max(i, epsilon) for i in y_predicted]\n",
    "    y_predicted_new = [min(i, 1-epsilon) for i in y_predicted_new]\n",
    "    y_predicted_new = np.clip(y_predicted_new, epsilon, 1 - epsilon)\n",
    "\n",
    "    return -np.mean(y_truth * np.log(y_predicted_new) + (1-y_truth) * np.log(1 - y_predicted_new) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98293bf5",
   "metadata": {},
   "source": [
    "# Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6ce38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_numpy(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4233ba25",
   "metadata": {},
   "source": [
    "# Gradient Descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bad526b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(age, affordability, y_true, epochs, loss_treshold):\n",
    "    w1 = w2 = 1\n",
    "    b = 0\n",
    "    rate = 0.5\n",
    "    n = len(age)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        weighted_sum = w1 * age + w2 * affordability + b\n",
    "        y_predicted = sigmoid_numpy(weighted_sum)\n",
    "        loss = logloss(y_true, y_predicted)\n",
    "        \n",
    "        w1_derivative = 1/n * (np.dot(np.transpose(age), (y_predicted - y_true)))\n",
    "        w2_derivative = 1/n * (np.dot(np.transpose(affordability), (y_predicted - y_true)))\n",
    "        \n",
    "        b_derivative = np.mean(y_predicted - y_true)\n",
    "        \n",
    "        w1 = w1 - rate * w1_derivative\n",
    "        w2 = w2 - rate * w2_derivative\n",
    "        \n",
    "        b = b - rate * b_derivative\n",
    "        \n",
    "        print(f\"Epoch {i}, w1: {w1}, w2: {w2}, b: {b}, loss: {loss}\")\n",
    "        \n",
    "        if loss <= loss_treshold:\n",
    "            break\n",
    "        \n",
    "    return w1, w2, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84403cc0",
   "metadata": {},
   "source": [
    "So these are the codes we did last Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8439b9",
   "metadata": {},
   "source": [
    "# Neural Network custom class\n",
    "\n",
    "Now, we want to create a class called **MyNeauralNetwork** where we will create methods such as **fit()** to train and **predict()** to predict\n",
    "\n",
    "1. So, first u create the class, \n",
    "\n",
    "2. initialize the constructor\n",
    "\n",
    "3. then we will put the above gradient descent func inside this class as a method\n",
    "\n",
    "4. then on top of that, will create the **fit()** method, now what does the **fit()** method takes as argument? X and y, but we will also add epochs and loss_treshold params\n",
    "\n",
    "5. inside this method will simply call the Gradient Descent method to figure out the optimal weights and bias, and then we will pass X['age'] and X['afforadibility'], y, epoch and loss_treshold\n",
    "\n",
    "6. Before anything, we need to initialize w1, w2 and b first, so we will simply initialize them to be 1 and 0 in the constructor\n",
    "\n",
    "7. So after running the Gradient Descent method, we will get w1,w2 and b back as a tuple, now we wanna store it in a variable to use it for prediction so we will store it in self.w1, self.w2 and self.b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af7ed2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNework:\n",
    "    def __init__(self):\n",
    "        self.w1 = 1\n",
    "        self.w2 = 1\n",
    "        self.b = 0\n",
    "        \n",
    "    def fit(self, X, y, epochs, loss_treshold):\n",
    "        self.w1, self.w2, self.b = self.gradient_descent(X['age'], X['affordibility'], y, epochs, loss_treshold)\n",
    "        \n",
    "    def gradient_descent(self, age, affordibility, y_true, epochs, loss_treshold):\n",
    "        w1 = w2 = 1\n",
    "        b = 0\n",
    "        learning_rate = 0.5\n",
    "        n = len(age)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            weighted_sum = w1 * age + w2 * affordibility + b\n",
    "            y_predicted = sigmoid_numpy(weighted_sum)\n",
    "            \n",
    "            loss = logloss(y_true, y_predicted)\n",
    "            \n",
    "            # Derivative for the weights and bias\n",
    "            w1_derivative = 1/n * np.dot(np.transpose(age), (y_predicted - y_true)) \n",
    "            w2_derivative = 1/n * np.dot(np.transpose(affordibility), (y_predicted - y_true)) \n",
    "            \n",
    "            b_derivative = np.mean(y_predicted - y_true)\n",
    "            \n",
    "            # update weights and bias using Derivative\n",
    "            w1 = w1 - learning_rate * w1_derivative\n",
    "            w2 = w2 - learning_rate * w2_derivative\n",
    "            \n",
    "            b = b - learning_rate * b\n",
    "            \n",
    "            print(f\"Epoch {i}, w1: {w1}, w2: {w2}, b: {b}, loss: {loss}\")\n",
    "            \n",
    "            if loss <= loss_treshold:\n",
    "                break\n",
    "        \n",
    "        return w1, w2, b\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec69d0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, w1: 0.9736899318847281, w2: 0.931388810977659, b: 0.0, loss: 0.7428288579142563\n",
      "Epoch 1, w1: 0.9492722836283222, w2: 0.8671452594841366, b: 0.0, loss: 0.7323761856384968\n",
      "Epoch 2, w1: 0.9266998810518501, w2: 0.8071711720521665, b: 0.0, loss: 0.723246404423731\n",
      "Epoch 3, w1: 0.9059133490480092, w2: 0.7513381912982909, b: 0.0, loss: 0.7153204802960138\n",
      "Epoch 4, w1: 0.8868432158992127, w2: 0.6994929143309135, b: 0.0, loss: 0.7084785787045061\n"
     ]
    }
   ],
   "source": [
    "obj = MyNeuralNework()\n",
    "obj.fit(X_train_scaled, y_train, epochs=5, loss_treshold=0.4923)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59af2eb4",
   "metadata": {},
   "source": [
    "Well, we can see that our custom Neural Network is working pretty fine, its doing 5 epochs as expextced.\n",
    "\n",
    "Now, lets do alil change since its gonna print after every epoch and when we do a big num of epoch its gonna be printing it alot of times and we dont want that,so what we will do is simply print only after every 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "14a21e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNework:\n",
    "    def __init__(self):\n",
    "        self.w1 = 1\n",
    "        self.w2 = 1\n",
    "        self.b = 0\n",
    "        \n",
    "    def fit(self, X, y, epochs, loss_treshold):\n",
    "        self.w1, self.w2, self.b = self.gradient_descent(X['age'], X['affordibility'], y, epochs, loss_treshold)\n",
    "        \n",
    "    def gradient_descent(self, age, affordibility, y_true, epochs, loss_treshold):\n",
    "        w1 = w2 = 1\n",
    "        b = 0\n",
    "        learning_rate = 0.5\n",
    "        n = len(age)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            weighted_sum = w1 * age + w2 * affordibility + b\n",
    "            y_predicted = sigmoid_numpy(weighted_sum)\n",
    "            \n",
    "            loss = logloss(y_true, y_predicted)\n",
    "            \n",
    "            # Derivative for the weights and bias\n",
    "            w1_derivative = 1/n * np.dot(np.transpose(age), (y_predicted - y_true)) \n",
    "            w2_derivative = 1/n * np.dot(np.transpose(affordibility), (y_predicted - y_true)) \n",
    "            \n",
    "            b_derivative = np.mean(y_predicted - y_true)\n",
    "            \n",
    "            # update weights and bias using Derivative\n",
    "            w1 = w1 - learning_rate * w1_derivative\n",
    "            w2 = w2 - learning_rate * w2_derivative\n",
    "            \n",
    "            b = b - learning_rate * b\n",
    "            \n",
    "            # Print only after every 50th epochs\n",
    "            if i % 50 == 0:\n",
    "                print(f\"Epoch {i}, w1: {w1}, w2: {w2}, b: {b}, loss: {loss}\")\n",
    "            \n",
    "            if loss <= loss_treshold:\n",
    "                # we will print these here as well so that we know when we break the loop whats the\n",
    "                # final weights and bias\n",
    "                print(f\"Epoch {i}, w1: {w1}, w2: {w2}, b: {b}, loss: {loss}\") \n",
    "                break\n",
    "        \n",
    "        return w1, w2, b\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba6f05",
   "metadata": {},
   "source": [
    "Now lets try 150 epochs and u will see it will only print after every 50th epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c7207f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, w1: 0.9736899318847281, w2: 0.931388810977659, b: 0.0, loss: 0.7428288579142563\n",
      "Epoch 50, w1: 0.7696392301040529, w2: 0.11676867907119683, b: 0.0, loss: 0.6708647224453433\n",
      "Epoch 100, w1: 0.8564645255590998, w2: 0.07222333663746555, b: 0.0, loss: 0.6704730037016706\n"
     ]
    }
   ],
   "source": [
    "obj = MyNeuralNework()\n",
    "obj.fit(X_train_scaled, y_train, epochs=150, loss_treshold=0.4923)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65cc0a4",
   "metadata": {},
   "source": [
    "Ok, now, we got the Gradient method to find the optimal weight and bias and we also got the fit method to train the NN, now lets create the predict method to allow for prediction\n",
    "\n",
    "We are done with the most important part which is determining the weight and bias "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa85490",
   "metadata": {},
   "source": [
    "# predict() method\n",
    "\n",
    "1. In predict method the argument will be the X_test\n",
    "2. We already know that, in any Neuron, there are 2 mathemtical components, one is Weighted sum and the other one is the Activation(Sigmoid), so we will be doing exactly that\n",
    "\n",
    "3. So, for Weighted sum u can simpy do self.w1 * X_test['age'] + self.w2 * X_test[''affordibility'] + self.b (We know that X_test is a Dataframe)\n",
    "\n",
    "4. Once u have ur Weighted sum u will simply call the sigmoid function and return it\n",
    "5. Also we will print the weigths and bias again while we break the loop to know the final weight and bias because it only print after every 50th epochs\n",
    "\n",
    "So yh, thats it, thats pretty much how u create a Neural Network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "854d42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNework:\n",
    "    def __init__(self):\n",
    "        # initializing weights and bias\n",
    "        self.w1 = 1\n",
    "        self.w2 = 1\n",
    "        self.b = 0\n",
    "        \n",
    "    # creating fit method for training\n",
    "    def fit(self, X, y, epochs, loss_treshold):\n",
    "        # calling the gradient_descent method and storing w1,w2 and b\n",
    "        self.w1, self.w2, self.b = self.gradient_descent(X['age'], X['affordibility'], y, \n",
    "                                                         epochs, loss_treshold)\n",
    "        print(f\"Final weights and bias: w1: {self.w1}, w2: {self.w2}, bias: {self.b}\")\n",
    "    \n",
    "    # creating predict method for prediction\n",
    "    def predict(self, X_test):\n",
    "        weighted_sum = self.w1 * X_test['age'] + self.w2 * X_test['affordibility'] + self.b\n",
    "        return sigmoid_numpy(weighted_sum)\n",
    "        \n",
    "    # gradient descent method to find optimal weights and bias\n",
    "    def gradient_descent(self, age, affordibility, y_true, epochs, loss_treshold):\n",
    "        w1 = w2 = 1\n",
    "        b = 0\n",
    "        learning_rate = 0.5\n",
    "        n = len(age)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            weighted_sum = w1 * age + w2 * affordibility + b\n",
    "            y_predicted = sigmoid_numpy(weighted_sum)\n",
    "            \n",
    "            loss = logloss(y_true, y_predicted)\n",
    "            \n",
    "            # Derivative for the weights and bias\n",
    "            w1_derivative = 1/n * (np.dot(np.transpose(age), (y_predicted - y_true)))\n",
    "            w2_derivative = 1/n * (np.dot(np.transpose(affordibility), (y_predicted - y_true)))\n",
    "            b_derivative = np.mean(y_predicted - y_true)\n",
    "            \n",
    "            # update weights and bias using Derivative and learning rate\n",
    "            w1 = w1 - learning_rate * w1_derivative\n",
    "            w2 = w2 - learning_rate * w2_derivative\n",
    "            b = b - learning_rate * b_derivative\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(f\"Epoch {i}, w1: {w1}, w2: {w2}, b: {b}, loss: {loss}\")\n",
    "            \n",
    "            if loss <= loss_treshold:\n",
    "                # we will print these here as well so that we know when we break the loop whats the\n",
    "                # final weights and bias\n",
    "                print(f\"Epoch {i}, w1: {w1}, w2: {w2}, b: {b}, loss: {loss}\") \n",
    "                break\n",
    "        \n",
    "        return w1, w2, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "66a51105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, w1: 0.9736899318847281, w2: 0.931388810977659, b: -0.11748951666770448, loss: 0.7428288579142563\n",
      "Epoch 50, w1: 1.524279872239132, w2: 0.8822187836689879, b: -1.1310088596841466, loss: 0.5943161377198863\n",
      "Epoch 100, w1: 2.2281870363373972, w2: 1.021207570480777, b: -1.5219348336892933, loss: 0.5674253359204207\n",
      "Epoch 150, w1: 2.8788802020633866, w2: 1.0918966282424585, b: -1.8376731064036336, loss: 0.5462065237139112\n",
      "Epoch 200, w1: 3.473286412049912, w2: 1.1354958313845407, b: -2.109494376761035, loss: 0.5289749850551891\n",
      "Epoch 250, w1: 4.014235251285109, w2: 1.168307211546194, b: -2.3515205001257873, loss: 0.5148317335261617\n",
      "Epoch 300, w1: 4.506491403088423, w2: 1.1967920767703706, b: -2.570834600613761, loss: 0.5031414005690805\n",
      "Epoch 350, w1: 4.955154772161112, w2: 1.2234470550440064, b: -2.7714756232427904, loss: 0.4934177492028928\n",
      "Epoch 357, w1: 5.01476871080517, w2: 1.2270942127849047, b: -2.7982358620493586, loss: 0.49219068663420595\n",
      "Final weights and bias: w1: 5.01476871080517, w2: 1.2270942127849047, bias: -2.7982358620493586\n"
     ]
    }
   ],
   "source": [
    "obj = MyNeuralNework()\n",
    "obj.fit(X_train_scaled, y_train, epochs=500, loss_treshold=0.4923)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1989f6a",
   "metadata": {},
   "source": [
    "We can see at 357 epoch we got our weight and bias value for almost close to the treshold\n",
    "\n",
    "If u go to the previous tutorial and check the optimal weights and bias which we got using Tensorflow, u will realize its very very similar to what we got!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696d5b6",
   "metadata": {},
   "source": [
    "Now, lets try the Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cb88349c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9     1\n",
       "25    1\n",
       "8     1\n",
       "21    0\n",
       "0     0\n",
       "12    0\n",
       "Name: bought_insurance, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f4b5d3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9     0.815758\n",
       "25    0.757098\n",
       "8     0.823176\n",
       "21    0.183263\n",
       "0     0.385115\n",
       "12    0.190888\n",
       "dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651e14a",
   "metadata": {},
   "source": [
    "We can see that our custom Neural Network predicted it perfectly (anything above 0.5 is yes else no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f481e5",
   "metadata": {},
   "source": [
    "Now, ofcours if u wanna write this class in a truly generic way, u will not have variable such as age and affordability as params in Gradient descent, u will instead have X (which will be ur dataframe), and u will use that Dataframe X instead of age and affordability and u will have w1 and w2 as a numpy vectors cuz we arent sure how many features there will be.\n",
    "\n",
    "Maybe in future tutorial will look into how to make that generic implementation, but this one is just to give u a good idea on how u can write ur own neural network class from scratch, this is something they may ask in ur data sceince or ml engineering interview so please practice this code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
